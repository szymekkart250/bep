{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "df = pd.read_csv('Agg_GridFlexHeetenDataset_full_10min.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower Bound: -0.06500000000596634, Upper Bound: 0.15100000000893485\n",
      "Length of the original dataframe:  7269187\n",
      "Length of the cleaned dataframe:  6511487\n",
      "Difference in length:  757700\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "\n",
    "Q1 = df['consumption_kwh'].quantile(0.25)\n",
    "Q3 = df['consumption_kwh'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n",
    "print('Length of the original dataframe: ', len(df))\n",
    "df_clean = df[(df['consumption_kwh'] >= lower_bound) & (df['consumption_kwh'] <= upper_bound)]\n",
    "print('Length of the cleaned dataframe: ', len(df_clean))\n",
    "print('Difference in length: ', str(len(df) - len(df_clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/rm1q12851jq6ww7xsgf4g0x00000gn/T/ipykernel_13825/429706352.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['time_10min'] = pd.to_datetime(df_clean['time_10min'])\n"
     ]
    }
   ],
   "source": [
    "df_clean['time_10min'] = pd.to_datetime(df_clean['time_10min'])\n",
    "df_group_10_min = df_clean.groupby('time_10min')['consumption_kwh'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_10_min['month'] = df_group_10_min['time_10min'].dt.month\n",
    "df_group_10_min['hour'] = df_group_10_min['time_10min'].dt.hour\n",
    "\n",
    "group_means = (\n",
    "    df_group_10_min[df_group_10_min['consumption_kwh'] >= 0]\n",
    "    .groupby(['month', 'hour'])['consumption_kwh']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'consumption_kwh': 'mean_consumption'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time_10min', 'consumption_kwh', 'month', 'hour', 'mean_consumption_x',\n",
       "       'mean_consumption_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group_10_min.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_10_min = df_group_10_min.merge(group_means, on=['month', 'hour'], how='left')\n",
    "df_group_10_min.loc[df_group_10_min['consumption_kwh'] < 0, 'consumption_kwh'] = df_group_10_min.loc[df_group_10_min['consumption_kwh'] < 0, 'mean_consumption']\n",
    "df_group_10_min.drop(columns=['mean_consumption', 'month', 'hour'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/rm1q12851jq6ww7xsgf4g0x00000gn/T/ipykernel_13825/50514365.py:7: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_group_10_min['consumption_kwh'], bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Cleaned Consumption Data')\n",
    "plt.xlabel('Consumption (kWh)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prices(price_dir: list[str]) -> pd.DataFrame:\n",
    "    all_files = glob.glob(os.path.join(price_dir, \"*.csv\"))\n",
    "    all_files = [f for f in all_files if f.startswith(price_dir + 'GUI') and f.endswith('.csv')]\n",
    "    dfs = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df['start_time'] = df['MTU (CET/CEST)'].apply(lambda x: x.split('-')[0].strip())\n",
    "    df['end_time'] = df['MTU (CET/CEST)'].apply(lambda x: x.split('-')[1].strip())\n",
    "    df['start_time'] = df['start_time'].astype(str).str.replace(r'\\(.*\\)', '', regex=True)\n",
    "    df['end_time'] = df['end_time'].astype(str).str.replace(r'\\(.*\\)', '', regex=True)\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'], format='mixed')\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'], format='mixed')\n",
    "    df = df[['Day-ahead Price (EUR/MWh)', 'start_time', 'end_time']]\n",
    "    df.sort_values(by='start_time', inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(df_consumption: pd.DataFrame, price_dir: str) -> pd.DataFrame:\n",
    "    df_c = df_consumption.copy()\n",
    "    df_c['time_10min'] = pd.to_datetime(df_c['time_10min'])\n",
    "    df_p = load_prices(price_dir)\n",
    "    df_c['year'] = df_c['time_10min'].dt.year\n",
    "    df_c['month'] = df_c['time_10min'].dt.month\n",
    "    df_c['day'] = df_c['time_10min'].dt.day\n",
    "    df_c['hour'] = df_c['time_10min'].dt.hour\n",
    "    df_p['year'] = df_p['start_time'].dt.year\n",
    "    df_p['month'] = df_p['start_time'].dt.month\n",
    "    df_p['day'] = df_p['start_time'].dt.day\n",
    "    df_p['hour'] = df_p['start_time'].dt.hour\n",
    "    merged = pd.merge(\n",
    "        df_c,\n",
    "        df_p,  \n",
    "        on=['year', 'month', 'day', 'hour'],\n",
    "        how='left',\n",
    "        suffixes=('_cons', '_price')\n",
    "    )\n",
    "    merged = merged.drop(columns=['year', 'month', 'day', 'hour'])\n",
    "    merged.to_csv('data/merged_data.csv', index=False)\n",
    "    merged['consumption_kwh'] = merged['consumption_kwh'].astype(float) * 30\n",
    "    return merged[['time_10min', 'consumption_kwh','Day-ahead Price (EUR/MWh)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = load_data(df_group_10_min, 'data_prices/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.rename(columns={'Day-ahead Price (EUR/MWh)': 'price_eur_per_kwh', 'time_10min':'timestamp'}, inplace=True)\n",
    "\n",
    "df_combined['price_eur_per_kwh'] = df_combined['price_eur_per_kwh'].astype(float) / 1000  # Convert EUR/MWh to EUR/kWh\n",
    "df_combined['consumption_kwh'] = df_combined['consumption_kwh'].astype(float) * 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('data/consumption_prices_10min.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>consumption_kwh</th>\n",
       "      <th>price_eur_per_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-08-01 02:00:00</td>\n",
       "      <td>960.75</td>\n",
       "      <td>0.01511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-08-01 02:10:00</td>\n",
       "      <td>1287.00</td>\n",
       "      <td>0.01511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-08-01 02:20:00</td>\n",
       "      <td>1238.40</td>\n",
       "      <td>0.01511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-08-01 02:30:00</td>\n",
       "      <td>1187.55</td>\n",
       "      <td>0.01511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-08-01 02:40:00</td>\n",
       "      <td>1087.20</td>\n",
       "      <td>0.01511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp  consumption_kwh  price_eur_per_kwh\n",
       "0 2018-08-01 02:00:00           960.75            0.01511\n",
       "1 2018-08-01 02:10:00          1287.00            0.01511\n",
       "2 2018-08-01 02:20:00          1238.40            0.01511\n",
       "3 2018-08-01 02:30:00          1187.55            0.01511\n",
       "4 2018-08-01 02:40:00          1087.20            0.01511"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4693951.06483048"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/consumption_prices_10min.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['price'] = df['price_eur_per_kwh'].astype(float)\n",
    "\n",
    "financial_result = sum(df['price'] *df['consumption_kwh'] * -1)\n",
    "financial_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Heuristic Trading Simulation Results ---\n",
      "Total Financial Result: $-4,435,484.10\n",
      "Total Battery Cycles: 1408.77\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def heuristic_trading_simulation(file_path):\n",
    "    \"\"\"\n",
    "    Simulates an energy trading strategy for a Tesla Megapack based on a heuristic approach.\n",
    "\n",
    "    This function loads a dataset of energy consumption and prices, and then simulates\n",
    "    the charging and discharging of a Tesla Megapack battery. The strategy is to buy\n",
    "    energy when the price is in the lowest 20% of the last 24 hours and sell when\n",
    "    it's in the highest 20%. It also prioritizes using the battery to meet\n",
    "    energy consumption needs.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file containing the energy data.\n",
    "                         The file should have 'timestamp', 'consumption', and 'price' columns.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - float: The total financial result from buying and selling energy.\n",
    "        - float: The total number of battery cycles during the simulation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "        data['price'] = data['price_eur_per_kwh']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file_path} was not found.\")\n",
    "        return None, None\n",
    "\n",
    "    megapack_capacity_kwh = 39160.0\n",
    "    megapack_power_kw = 9790.0\n",
    "    charge_per_10_min = megapack_power_kw * (10 / 60)\n",
    "    battery_soc_kwh = 0.0 \n",
    "    financial_result = 0.0\n",
    "    total_charged = 0.0\n",
    "    buy_history = []\n",
    "    sell_history = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        past_24h = data[data['timestamp'] < data['timestamp'][i]].tail(144)\n",
    "        if not past_24h.empty:\n",
    "            low_price_threshold = past_24h['price'].quantile(0.3)\n",
    "            high_price_threshold = past_24h['price'].quantile(0.8)\n",
    "        else:\n",
    "            low_price_threshold = data['price'][i]\n",
    "            high_price_threshold = data['price'][i]\n",
    "            \n",
    "        current_price = data['price'][i]\n",
    "        consumption = data['consumption_kwh'][i]\n",
    "        if battery_soc_kwh >= consumption:\n",
    "            battery_soc_kwh -= consumption\n",
    "        else:\n",
    "            grid_pull = consumption - battery_soc_kwh\n",
    "            battery_soc_kwh = 0\n",
    "            financial_result -= grid_pull * current_price\n",
    "            buy_history.append(grid_pull * current_price)\n",
    "\n",
    "        if current_price <= low_price_threshold:\n",
    "            charge_amount = min(charge_per_10_min, megapack_capacity_kwh - battery_soc_kwh)\n",
    "            battery_soc_kwh += charge_amount\n",
    "            financial_result -= charge_amount * current_price\n",
    "            buy_history.append(charge_amount * current_price)\n",
    "            total_charged += charge_amount\n",
    "            \n",
    "        elif current_price >= high_price_threshold:\n",
    "            discharge_amount = min(charge_per_10_min, battery_soc_kwh)\n",
    "            battery_soc_kwh -= discharge_amount\n",
    "            financial_result += discharge_amount * current_price\n",
    "            sell_history.append(discharge_amount * current_price)\n",
    "\n",
    "    battery_cycles = total_charged / megapack_capacity_kwh if megapack_capacity_kwh > 0 else 0\n",
    "    return financial_result, battery_cycles\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_path = 'data/consumption_prices_10min.csv'\n",
    "    final_profit, total_cycles = heuristic_trading_simulation(dataset_path)\n",
    "    if final_profit is not None and total_cycles is not None:\n",
    "        print(\"--- Heuristic Trading Simulation Results ---\")\n",
    "        print(f\"Total Financial Result: ${final_profit:,.2f}\")\n",
    "        print(f\"Total Battery Cycles: {total_cycles:.2f}\")\n",
    "        print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(258466.9638935551)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_profit_over_baseline = final_profit - financial_result\n",
    "total_profit_over_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/rm1q12851jq6ww7xsgf4g0x00000gn/T/ipykernel_13825/1733437982.py:30: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1/50:   0%|          | 0/105543 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=float, device=mps:0, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 365\u001b[39m\n\u001b[32m    361\u001b[39m     plt.show()\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     trained_ppo, rewards = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m     evaluate(trained_ppo, df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 270\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    268\u001b[39m time_step += \u001b[32m1\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# Running policy_old:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m action, logprob = ppo.policy_old.act(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m.to(device))\n\u001b[32m    271\u001b[39m state, reward, done, _ = env.step(action)\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Saving reward and is_terminals:\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: expected TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=float, device=mps:0, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "df = pd.read_csv('data/consumption_prices_10min.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['price_lag_1'] = df['price_eur_per_kwh'].shift(1)\n",
    "df['price_lag_6'] = df['price_eur_per_kwh'].shift(6)\n",
    "df['price_ma_24h'] = df['price_eur_per_kwh'].rolling(window=144).mean()\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "CAP_KWH             = 39160.0   \n",
    "MAX_PWR_KW          =   9790.0  \n",
    "class BatteryEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gym environment for the battery energy trading problem.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, battery_capacity_kwh=39160, max_charge_rate_kw=9790.0, max_discharge_rate_kw=9790.0, efficiency=1.0):\n",
    "        super(BatteryEnv, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.battery_capacity_kwh = battery_capacity_kwh\n",
    "        self.max_charge_energy = max_charge_rate_kw * (10/60)\n",
    "        self.max_discharge_energy = max_discharge_rate_kw * (10/60)\n",
    "        self.efficiency = efficiency\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(10,), dtype=np.float32)\n",
    "        self.current_step = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.state_of_charge = self.battery_capacity_kwh / 2\n",
    "        self.cumulative_profit = 0\n",
    "        self.total_cycles = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        soc_normalized = (self.state_of_charge / self.battery_capacity_kwh) * 2 - 1\n",
    "        data_row = self.df.iloc[self.current_step]\n",
    "        \n",
    "        # Normalize all features for the NN\n",
    "        price_mean = self.df['price_eur_per_kwh'].mean()\n",
    "        price_std = self.df['price_eur_per_kwh'].std()\n",
    "        consumption_mean = self.df['consumption_kwh'].mean()\n",
    "        consumption_std = self.df['consumption_kwh'].std()\n",
    "\n",
    "        price_normalized = (data_row['price_eur_per_kwh'] - price_mean) / price_std\n",
    "        consumption_normalized = (data_row['consumption_kwh'] - consumption_mean) / consumption_std\n",
    "        price_lag_1_normalized = (data_row['price_lag_1'] - price_mean) / price_std\n",
    "        price_lag_6_normalized = (data_row['price_lag_6'] - price_mean) / price_std\n",
    "        price_ma_normalized = (data_row['price_ma_24h'] - price_mean) / price_std\n",
    "\n",
    "        obs = np.array([\n",
    "            soc_normalized,\n",
    "            price_normalized,\n",
    "            consumption_normalized,\n",
    "            data_row['hour_sin'],\n",
    "            data_row['hour_cos'],\n",
    "            data_row['day_of_week_sin'],\n",
    "            data_row['day_of_week_cos'],\n",
    "            price_lag_1_normalized,\n",
    "            price_lag_6_normalized,\n",
    "            price_ma_normalized\n",
    "        ], dtype=np.float32)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        price = self.df['price_eur_per_kwh'].iloc[self.current_step]\n",
    "        consumption = self.df['consumption_kwh'].iloc[self.current_step]\n",
    "        # --- START OF CHANGE 3: Revamped Reward Function ---\n",
    "        price_baseline = self.df['price_ma_24h'].iloc[self.current_step] # Use MA as baseline\n",
    "        reward = 0\n",
    "\n",
    "        # 1. Apply consumption\n",
    "        self.state_of_charge -= consumption\n",
    "        if self.state_of_charge < 0:\n",
    "            energy_needed = -self.state_of_charge\n",
    "            self.cumulative_profit -= energy_needed * price # Real cost\n",
    "            self.state_of_charge = 0\n",
    "            reward -= 50 # High penalty for forced grid purchase\n",
    "\n",
    "        # 2. Execute agent's action\n",
    "        if action == 0:  # Sell\n",
    "            energy_to_sell = min(self.max_discharge_energy, self.state_of_charge)\n",
    "            if energy_to_sell > 0:\n",
    "                self.state_of_charge -= energy_to_sell\n",
    "                revenue = energy_to_sell * price\n",
    "                self.cumulative_profit += revenue\n",
    "                \n",
    "                # Reward is based on selling above the baseline price\n",
    "                reward += (price - price_baseline) * energy_to_sell\n",
    "                \n",
    "                # Apply degradation cost\n",
    "                cycle_fraction = energy_to_sell / self.battery_capacity_kwh\n",
    "                self.total_cycles += cycle_fraction\n",
    "                reward -= cycle_fraction * self.battery_capacity_kwh\n",
    "\n",
    "        elif action == 1:  # Buy\n",
    "            energy_to_buy = min(self.max_charge_energy, self.battery_capacity_kwh - self.state_of_charge)\n",
    "            if energy_to_buy > 0:\n",
    "                energy_added = energy_to_buy * self.efficiency\n",
    "                self.state_of_charge += energy_added\n",
    "                cost = energy_to_buy * price\n",
    "                self.cumulative_profit -= cost\n",
    "                \n",
    "                # Reward is based on buying below the baseline price\n",
    "                reward += (price_baseline - price) * energy_to_buy\n",
    "                \n",
    "                # Apply degradation cost\n",
    "                cycle_fraction = energy_to_buy / self.battery_capacity_kwh\n",
    "                self.total_cycles += cycle_fraction\n",
    "                reward -= cycle_fraction * self.battery_capacity_kwh\n",
    "        \n",
    "        elif action == 2: # Hold\n",
    "            pass        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "\n",
    "        info = {\n",
    "            'cumulative_profit': self.cumulative_profit,\n",
    "            'state_of_charge': self.state_of_charge,\n",
    "            'total_cycles': self.total_cycles\n",
    "        }\n",
    "\n",
    "        return self._get_observation(), reward, done, info\n",
    "    \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        return action.item(), action_logprob\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_value = self.critic(state)\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def update(self, memory):\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(memory.states).detach()).to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(memory.actions).detach()).to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs).detach()).to(device)\n",
    "        for _ in range(self.K_epochs):\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.05*dist_entropy\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "def train():\n",
    "    # Hyperparameters\n",
    "    update_timestep = 1024 # update policy every n timesteps\n",
    "    K_epochs = 80          # update policy for K epochs\n",
    "    eps_clip = 0.2         # clip parameter for PPO\n",
    "    gamma = 0.99           # discount factor\n",
    "    lr = 0.0003            # learning rate for adam\n",
    "    max_episodes = 50      # number of episodes to train for\n",
    "\n",
    "    env = BatteryEnv(df)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    memory = Memory()\n",
    "    ppo = PPO(state_dim, action_dim, lr, gamma, K_epochs, eps_clip)\n",
    "\n",
    "    time_step = 0\n",
    "    episode_rewards = []\n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    for i_episode in range(1, max_episodes + 1):\n",
    "        state = env.reset()\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        episode_reward = 0\n",
    "        for t in tqdm(range(len(df)-1), desc=f\"Episode {i_episode}/{max_episodes}\"):\n",
    "            time_step += 1\n",
    "            # Running policy_old:\n",
    "            action, logprob = ppo.policy_old.act(torch.FloatTensor(state).to(device))\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            # Saving reward and is_terminals:\n",
    "            memory.actions.append(torch.tensor(action))\n",
    "            memory.states.append(torch.FloatTensor(state))\n",
    "            memory.logprobs.append(logprob)\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "\n",
    "            # update if its time\n",
    "            if time_step % update_timestep == 0:\n",
    "                ppo.update(memory)\n",
    "                memory.clear_memory()\n",
    "                time_step = 0\n",
    "                \n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"Episode {i_episode} \\t Last reward: {episode_reward:.2f}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    return ppo, episode_rewards\n",
    "\n",
    "# --- 5. Evaluation and Visualization ---\n",
    "def evaluate(ppo_agent, df):\n",
    "    print(\"\\nStarting Evaluation...\")\n",
    "    env = BatteryEnv(df)\n",
    "    state = env.reset()\n",
    "    \n",
    "    history = {\n",
    "        'profit': [],\n",
    "        'soc': [],\n",
    "        'action': [],\n",
    "        'price': [],\n",
    "        'consumption': []\n",
    "    }\n",
    "    \n",
    "    for t in tqdm(range(len(df)-1), desc=\"Evaluating\"):\n",
    "        action, _ = ppo_agent.policy.act(torch.FloatTensor(state))\n",
    "        state, _, done, info = env.step(action)\n",
    "        \n",
    "        history['profit'].append(info['cumulative_profit'])\n",
    "        history['soc'].append(info['state_of_charge'])\n",
    "        history['action'].append(action)\n",
    "        history['price'].append(df['price_eur_per_kwh'].iloc[t])\n",
    "        history['consumption'].append(df['consumption_kwh'].iloc[t])\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Final performance metrics\n",
    "    final_profit = history['profit'][-1]\n",
    "    final_cycles = env.total_cycles\n",
    "    print(f\"\\nEvaluation Complete.\")\n",
    "    print(f\"Final Cumulative Profit: {final_profit:.2f} EUR\")\n",
    "    print(f\"Total Battery Cycles: {final_cycles:.2f}\")\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(15, 12), sharex=True)\n",
    "    fig.suptitle('PPO Agent Performance Evaluation', fontsize=16)\n",
    "\n",
    "    axs[0].plot(history['profit'], label='Cumulative Profit', color='green')\n",
    "    axs[0].set_ylabel('Profit (EUR)')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    axs[1].plot(history['soc'], label='State of Charge (SoC)', color='blue')\n",
    "    axs[1].set_ylabel('SoC (kWh)')\n",
    "    axs[1].legend(loc='upper left')\n",
    "    \n",
    "    ax2 = axs[1].twinx()\n",
    "    buy_indices = [i for i, a in enumerate(history['action']) if a == 1]\n",
    "    sell_indices = [i for i, a in enumerate(history['action']) if a == 0]\n",
    "    ax2.plot(buy_indices, [history['price'][i] for i in buy_indices], 'go', markersize=4, label='Buy Action')\n",
    "    ax2.plot(sell_indices, [history['price'][i] for i in sell_indices], 'ro', markersize=4, label='Sell Action')\n",
    "    ax2.set_ylabel('Price at Action (EUR/kWh)')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    axs[2].plot(history['price'], label='Market Price', color='orange', alpha=0.7)\n",
    "    axs[2].set_ylabel('Price (EUR/kWh)')\n",
    "    ax3 = axs[2].twinx()\n",
    "    ax3.plot(history['consumption'], label='Consumption', color='purple', alpha=0.6)\n",
    "    ax3.set_ylabel('Consumption (kWh)')\n",
    "    axs[2].set_xlabel('Timestep (10 min intervals)')\n",
    "    lines, labels = axs[2].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax3.get_legend_handles_labels()\n",
    "    axs[2].legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "    axs[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trained_ppo, rewards = train()\n",
    "    evaluate(trained_ppo, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in ./env/lib/python3.13/site-packages (from gym) (2.2.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./env/lib/python3.13/site-packages (from gym) (3.1.1)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827725 sha256=4332498b759cf45fa851af34dad0c188b21338a7027959d0e5b5d7b2bac8e293\n",
      "  Stored in directory: /Users/szymonkozak/Library/Caches/pip/wheels/1d/34/c6/856a1e1eff47d8f18545c833b6138ae1e9f53c7de9bcc5f31d\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, gym\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2/2\u001b[0m [gym]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gym-0.26.2 gym_notices-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/rm1q12851jq6ww7xsgf4g0x00000gn/T/ipykernel_13825/286128804.py:35: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      " Training \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1/50:  48%|     | 51019/105543 [06:11<06:36, 137.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[126]\u001b[39m\u001b[32m, line 389\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;66;03m# ------------------ 7. MAIN ------------------\u001b[39;00m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     agent, rewards = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m     evaluate(agent, df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[126]\u001b[39m\u001b[32m, line 283\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    280\u001b[39m timestep += \u001b[32m1\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;66;03m#  Select action\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m action, logprob = \u001b[43mppo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpolicy_old\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m#  Step environment\u001b[39;00m\n\u001b[32m    286\u001b[39m next_state_np, reward, done, _ = env.step(action)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[126]\u001b[39m\u001b[32m, line 178\u001b[39m, in \u001b[36mActorCritic.act\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: torch.Tensor):\n\u001b[32m    177\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Given a state (tensor on the same device), sample an action.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     dist = Categorical(probs)\n\u001b[32m    180\u001b[39m     action = dist.sample()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TaKo/tiktoczki/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TaKo/tiktoczki/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TaKo/tiktoczki/env/lib/python3.13/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TaKo/tiktoczki/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TaKo/tiktoczki/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/TaKo/tiktoczki/env/lib/python3.13/site-packages/torch/nn/modules/activation.py:392\u001b[39m, in \u001b[36mTanh.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else (\n",
    "    'cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "df = pd.read_csv('data/consumption_prices_10min.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['price_lag_1'] = df['price_eur_per_kwh'].shift(1)\n",
    "df['price_lag_6'] = df['price_eur_per_kwh'].shift(6)\n",
    "df['price_ma_24h'] = df['price_eur_per_kwh'].rolling(window=144).mean()\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "CAP_KWH = 39160.0\n",
    "MAX_PWR_KW = 9790.0\n",
    "\n",
    "\n",
    "class BatteryEnv(gym.Env):\n",
    "    \"\"\"Custom Gym environment for the battery energy trading problem.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe: pd.DataFrame,\n",
    "                 battery_capacity_kwh: float = CAP_KWH,\n",
    "                 max_charge_rate_kw: float = MAX_PWR_KW,\n",
    "                 max_discharge_rate_kw: float = MAX_PWR_KW,\n",
    "                 efficiency: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.df = dataframe\n",
    "        self.battery_capacity_kwh = battery_capacity_kwh\n",
    "        self.max_charge_energy = max_charge_rate_kw * (10 / 60)  # 10-min slot\n",
    "        self.max_discharge_energy = max_discharge_rate_kw * (10 / 60)\n",
    "        self.efficiency = efficiency\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)  # 0-Sell, 1-Buy, 2-Hold\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(10,), dtype=np.float32)\n",
    "\n",
    "        self.current_step: int = 0\n",
    "        self.state_of_charge: float = 0.0\n",
    "        self.cumulative_profit: float = 0.0\n",
    "        self.total_cycles: float = 0.0\n",
    "        self.reset()\n",
    "\n",
    "    def _get_observation(self) -> np.ndarray:\n",
    "        soc_norm = (self.state_of_charge / self.battery_capacity_kwh) * 2 - 1\n",
    "        row = self.df.iloc[self.current_step]\n",
    "\n",
    "        price_mean = self.df['price_eur_per_kwh'].mean()\n",
    "        price_std = self.df['price_eur_per_kwh'].std()\n",
    "        cons_mean = self.df['consumption_kwh'].mean()\n",
    "        cons_std = self.df['consumption_kwh'].std()\n",
    "\n",
    "        price_norm = (row['price_eur_per_kwh'] - price_mean) / price_std\n",
    "        cons_norm = (row['consumption_kwh'] - cons_mean) / cons_std\n",
    "        lag1_norm = (row['price_lag_1'] - price_mean) / price_std\n",
    "        lag6_norm = (row['price_lag_6'] - price_mean) / price_std\n",
    "        ma_norm = (row['price_ma_24h'] - price_mean) / price_std\n",
    "\n",
    "        obs = np.array([\n",
    "            soc_norm,\n",
    "            price_norm,\n",
    "            cons_norm,\n",
    "            row['hour_sin'],\n",
    "            row['hour_cos'],\n",
    "            row['day_of_week_sin'],\n",
    "            row['day_of_week_cos'],\n",
    "            lag1_norm,\n",
    "            lag6_norm,\n",
    "            ma_norm,\n",
    "        ], dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.state_of_charge = self.battery_capacity_kwh / 2\n",
    "        self.cumulative_profit = 0.0\n",
    "        self.total_cycles = 0.0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        price = self.df['price_eur_per_kwh'].iloc[self.current_step]\n",
    "        consumption = self.df['consumption_kwh'].iloc[self.current_step]\n",
    "        price_baseline = self.df['price_ma_24h'].iloc[self.current_step]\n",
    "\n",
    "        # 1 Apply consumption\n",
    "        self.state_of_charge -= consumption\n",
    "        reward = 0.0\n",
    "        if self.state_of_charge < 0:\n",
    "            grid_energy = -self.state_of_charge\n",
    "            self.cumulative_profit -= grid_energy * price\n",
    "            self.state_of_charge = 0\n",
    "            reward -= 50  # large penalty\n",
    "\n",
    "        # 2 Agent action\n",
    "        if action == 0:  # Sell\n",
    "            sell = min(self.max_discharge_energy, self.state_of_charge)\n",
    "            if sell > 0:\n",
    "                self.state_of_charge -= sell\n",
    "                self.cumulative_profit += sell * price\n",
    "                reward += (price - price_baseline) * sell\n",
    "                cycle = sell / self.battery_capacity_kwh\n",
    "                self.total_cycles += cycle\n",
    "                reward -= cycle * self.battery_capacity_kwh\n",
    "        elif action == 1:  # Buy\n",
    "            buy = min(self.max_charge_energy, self.battery_capacity_kwh - self.state_of_charge)\n",
    "            if buy > 0:\n",
    "                added = buy * self.efficiency\n",
    "                self.state_of_charge += added\n",
    "                self.cumulative_profit -= buy * price\n",
    "                reward += (price_baseline - price) * buy\n",
    "                cycle = buy / self.battery_capacity_kwh\n",
    "                self.total_cycles += cycle\n",
    "                reward -= cycle * self.battery_capacity_kwh\n",
    "        # Hold  nothing extra\n",
    "\n",
    "        # 3 Advance\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "        info = {\n",
    "            'cumulative_profit': self.cumulative_profit,\n",
    "            'state_of_charge': self.state_of_charge,\n",
    "            'total_cycles': self.total_cycles,\n",
    "        }\n",
    "        return self._get_observation(), reward, done, info\n",
    "\n",
    "\n",
    "# ------------------ 3. ACTORCRITIC NETWORK ------------------\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def act(self, state: torch.Tensor):\n",
    "        \"\"\"Given a state (tensor on the same device), sample an action.\"\"\"\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "    def evaluate(self, states, actions):\n",
    "        probs = self.actor(states)\n",
    "        dist = Categorical(probs)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        values = self.critic(states).squeeze(-1)\n",
    "        return log_probs, values, entropy\n",
    "\n",
    "\n",
    "# ------------------ 4. PPO AGENT ------------------\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.__init__()\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma, K_epochs, eps_clip):\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.value_loss = nn.MSELoss()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    def update(self, memory: Memory):\n",
    "        # 1 Compute returns\n",
    "        rewards = []\n",
    "        discounted = 0.0\n",
    "        for r, done in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if done:\n",
    "                discounted = 0.0\n",
    "            discounted = r + self.gamma * discounted\n",
    "            rewards.insert(0, discounted)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # 2 Pack rollouts\n",
    "        states = torch.stack(memory.states).to(device).detach()\n",
    "        actions = torch.tensor(memory.actions, device=device)\n",
    "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
    "\n",
    "        # 3 Optimize policy\n",
    "        for _ in range(self.K_epochs):\n",
    "            logprobs, state_values, entropy = self.policy.evaluate(states, actions)\n",
    "            ratios = torch.exp(logprobs - old_logprobs)\n",
    "            advantages = rewards - state_values.detach()\n",
    "\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.value_loss(state_values, rewards) - 0.05 * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "\n",
    "# ------------------ 5. TRAINING ------------------\n",
    "def train():\n",
    "    # Hyper-parameters\n",
    "    UPDATE_EVERY = 1024\n",
    "    K_EPOCHS = 80\n",
    "    GAMMA = 0.99\n",
    "    EPS_CLIP = 0.2\n",
    "    LR = 3e-4\n",
    "    EPISODES = 50\n",
    "\n",
    "    env = BatteryEnv(df)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    memory = Memory()\n",
    "    ppo = PPO(state_dim, action_dim, LR, GAMMA, K_EPOCHS, EPS_CLIP)\n",
    "\n",
    "    timestep = 0\n",
    "    all_rewards = []\n",
    "\n",
    "    print(\"\\n Training \")\n",
    "    for ep in range(1, EPISODES + 1):\n",
    "        state_np = env.reset()\n",
    "        state = torch.from_numpy(state_np).float().to(device)\n",
    "        ep_reward = 0.0\n",
    "\n",
    "        for _ in tqdm(range(len(df) - 1), desc=f\"Episode {ep}/{EPISODES}\"):\n",
    "            timestep += 1\n",
    "\n",
    "            #  Select action\n",
    "            action, logprob = ppo.policy_old.act(state)\n",
    "\n",
    "            #  Step environment\n",
    "            next_state_np, reward, done, _ = env.step(action)\n",
    "\n",
    "            #  Store rollout\n",
    "            memory.states.append(state)\n",
    "            memory.actions.append(action)\n",
    "            memory.logprobs.append(logprob.detach())\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "\n",
    "            ep_reward += reward\n",
    "            state = torch.from_numpy(next_state_np).float().to(device)\n",
    "\n",
    "            #  Update PPO\n",
    "            if timestep % UPDATE_EVERY == 0:\n",
    "                ppo.update(memory)\n",
    "                memory.clear()\n",
    "                timestep = 0\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(ep_reward)\n",
    "        print(f\"Episode {ep}: reward = {ep_reward:.2f}\")\n",
    "\n",
    "    print(\"Training complete.\\n\")\n",
    "    return ppo, all_rewards\n",
    "\n",
    "\n",
    "# ------------------ 6. EVALUATION ------------------\n",
    "def evaluate(agent: PPO, dataframe: pd.DataFrame):\n",
    "    print(\"\\n Evaluation \")\n",
    "    env = BatteryEnv(dataframe)\n",
    "    state = torch.from_numpy(env.reset()).float().to(device)\n",
    "\n",
    "    history = {\n",
    "        'profit': [],\n",
    "        'soc': [],\n",
    "        'action': [],\n",
    "        'price': [],\n",
    "        'consumption': [],\n",
    "    }\n",
    "\n",
    "    for t in tqdm(range(len(dataframe) - 1)):\n",
    "        action, _ = agent.policy.act(state)\n",
    "        next_state_np, _, done, info = env.step(action)\n",
    "\n",
    "        # Log\n",
    "        history['profit'].append(info['cumulative_profit'])\n",
    "        history['soc'].append(info['state_of_charge'])\n",
    "        history['action'].append(action)\n",
    "        history['price'].append(dataframe['price_eur_per_kwh'].iloc[t])\n",
    "        history['consumption'].append(dataframe['consumption_kwh'].iloc[t])\n",
    "\n",
    "        state = torch.from_numpy(next_state_np).float().to(device)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Summary\n",
    "    print(f\"Final profit: {history['profit'][-1]:.2f} EUR\")\n",
    "    print(f\"Total cycles: {env.total_cycles:.2f}\\n\")\n",
    "\n",
    "    # Plots (CPU arrays)\n",
    "    profit = np.array(history['profit'])\n",
    "    soc = np.array(history['soc'])\n",
    "    price = np.array(history['price'])\n",
    "    cons = np.array(history['consumption'])\n",
    "    actions = np.array(history['action'])\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "    fig.suptitle('PPO Agent Performance', fontsize=16)\n",
    "\n",
    "    # Profit\n",
    "    axs[0].plot(profit)\n",
    "    axs[0].set_ylabel('Cumulative profit (EUR)')\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # SoC + actions on second panel\n",
    "    axs[1].plot(soc, label='SoC (kWh)')\n",
    "    axs[1].set_ylabel('State of charge')\n",
    "    buy_idx = np.where(actions == 1)[0]\n",
    "    sell_idx = np.where(actions == 0)[0]\n",
    "    axs[1].scatter(buy_idx, soc[buy_idx], marker='^', label='Buy', s=10)\n",
    "    axs[1].scatter(sell_idx, soc[sell_idx], marker='v', label='Sell', s=10)\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Price & consumption\n",
    "    axs[2].plot(price, label='Price (EUR/kWh)')\n",
    "    ax2 = axs[2].twinx()\n",
    "    ax2.plot(cons, color='tab:orange', label='Consumption (kWh)', alpha=0.5)\n",
    "    axs[2].set_xlabel('Timestep (10-min)')\n",
    "    axs[2].grid(True)\n",
    "\n",
    "    lines1, labels1 = axs[2].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    axs[2].legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------ 7. MAIN ------------------\n",
    "if __name__ == '__main__':\n",
    "    agent, rewards = train()\n",
    "    evaluate(agent, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
